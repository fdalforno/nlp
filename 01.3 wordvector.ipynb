{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c31bb89",
   "metadata": {},
   "source": [
    "# Skip-gram Word2Vec\n",
    "In questo notebook, andremo ad utilizzare PyTorch per implementare un [algoritmo Word2Vec](https://en.wikipedia.org/wiki/Word2vec) usando l'architettura skip-gram. Implementando quessto impareremo come funziona il words embedding nel natural language processing. \n",
    "\n",
    "\n",
    "Qui abbiamo delle risorse che si possono consultare per capire meglio cosa andremo a fare. La lettura di questi documenti aiuta a capire bene questo documento.\n",
    "\n",
    "* Una ottima [introduzione concettuale](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) del Word2Vec di Chris McCormick\n",
    "* Il primo paper [Word2Vec](./docs/NIPS-2013-distributed-representations-of-words-and-phrases-and-their-compositionality-Paper.pdf) di Mikolov e altri.\n",
    "\n",
    "---\n",
    "## Word embeddings\n",
    "\n",
    "Quando si ha a che fare con le parole nel testo, si finisce con una decina di migliaia di classi di parole di analizzare; una per ogni classe di parole del vocabolario. Cercare di codificare queste parole tramite vettore one-hot è estremamente inefficiente perchè molti valori nel vettore one-hot è impostata a zero. Dunque la moltiplicazione con la matrice che avviene tra il vettore one-hot e il primo hidden layer avrà molti valori impostati a zero.\n",
    "\n",
    "<img src='images/one_hot_encoding.png' width=50%>\n",
    "\n",
    "Per risolvere questo problema e aumentare l'efficienza delle nostre reti, andremo ad usare quello che chiamiamo **embeddings**.\n",
    "Il livello di embeddings sostituisce le moltiplicazioni che la nostra rete dovrebbe fare con i vettori one-hot, concettualmente andiamo a togliere quelle moltiplicazioni e andiamo a recuperare direttamente i pesi che la rete avrebbe appreso.\n",
    "\n",
    "<img src='images/lookup_matrix.png' width=50%>\n",
    "\n",
    "Invece di eseguire la moltiplicazione della matrice, usiamo la matrice dei pesi come una tabella di lookup.\n",
    "Codifichiamo le parole come interi, per esempio \"heart\" è codificato come 985, \"mind\" come 18094. Poi per ottenere il valore dell'hidden layer basta prendere il valore alla riga 958 della matrice di embedding. Questo processo è chiamato **embedding lookup** e il numero delle hidden unit è chiamato anche **embedding dimension**.\n",
    "\n",
    "<img src='images/tokenize_lookup.png' width=50%>\n",
    "\n",
    "Non c'è nulla di magico in quello che stiamo facendo. La tabella di lookup degli embeddings è solatemente una matrice di pesi e il lookup è solamente una scorciatoia per la moltiplicazione della matrice. La tabella di lookup è aggiornata come una matrice di pesi.\n",
    "\n",
    "Gli embeddings non sono solo usati con le parole, ovviamente. Possiamo usarli su un qualsiasi modello che ha un numero elevato di classi. Un particolare tipo di modello è chiamato **Word2Vec** usa il layer di embedding per trovare una rappresentazione vettoriale che contiene un signficato semantico. \n",
    "\n",
    "---\n",
    "## Word2Vec\n",
    "\n",
    "L'algoritmo Word2Vec trova rappresentazioni molto effcienti di vettori che rappresentano una parola. Questi vettori contengono anche informazioni semantiche circa le parole.\n",
    "\n",
    "\n",
    "<img src=\"images/context_drink.png\" width=40%>\n",
    "\n",
    "Le parole che compaiono in **contesti** simili ad esempio \"coffee\",\"tea\" e \"water\" avranno vettori vicini l'uno all'altro.\n",
    "Parole differenti saranno più lontane dalle altre e le relazioni potranno essere rappresentate dalla distanza nello spazio vettoriale.\n",
    "\n",
    "<img src=\"images/vector_distance.png\" width=40%>\n",
    "\n",
    "come detto prima ci sono due architetture per implementare il Word2Vec\n",
    "\n",
    "* CBOW (Continuous Bag-Of-Words) \n",
    "* Skip-gram\n",
    "\n",
    "<img src=\"images/word2vec_architectures.png\" width=60%>\n",
    "\n",
    "In questa implementazione, useremo l'architettura **skip-gram** in quanto da risultati migliori della CBOW.\n",
    "Al nostro modello daremo in pasto una parola e proveremo a predire le parole che la circondano nel testo.\n",
    "In questo modo, possiamo eseguire l'addestramento della rete per ottenere rappresentazioni di parole che compaioni in contesti simili.\n",
    "\n",
    "---\n",
    "## Caricamento dei dati\n",
    "\n",
    "Andiamo a caricare i dati nella cartella `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25783933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 's3.amazonaws.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae42a4c69f6493d92dbcd9b845533f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31344016 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "dataset = \"https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bbe6499_text8/text8.zip\"\n",
    "folder_result = \"./data/\" \n",
    "\n",
    "with requests.get(dataset, stream=True, verify=False) as r:\n",
    "    total_length = int(r.headers.get(\"Content-Length\"))\n",
    "    with tqdm.wrapattr(r.raw, \"read\", total=total_length, desc=\"\")as raw:\n",
    "        with open(f\"{os.path.join(folder_result,os.path.basename(r.url))}\", 'wb') as output:\n",
    "            shutil.copyfileobj(raw, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1577bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"./data/text8.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(folder_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45d5f5d",
   "metadata": {},
   "source": [
    "visualizziamo ora il testo caricato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dd2572d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including t\n"
     ]
    }
   ],
   "source": [
    "# read in the extracted text file      \n",
    "with open('data/text8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# print out the first 100 characters\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01500194",
   "metadata": {},
   "source": [
    "## Preprocessamento\n",
    "\n",
    "Andiamo a sistemare il testo per facilitare la fase di trainig. Ci viene in aiuto il file `utils.py`.\n",
    "La funzione `preprocess` fa queste poche cose:\n",
    "\n",
    "* Converte la punteggiatura in tokens, dunque una virgola viene cambiata in ` <PERIOD> `. In questo dataset non c'è punteggiatura ma questo può aiutare in altri problemi di NLP.\n",
    "\n",
    "* Rimuove tutte le parole che compaiono 5 o *meno* volte nel dataset. Questo andra a ridurre i problemi legati al rumore nei dati e aumenta la qualità della rappresentazione vettoriale.\n",
    "\n",
    "\n",
    "* Restituisce una lista di parole del testo.\n",
    "\n",
    "Questo prende qualche secondo per eseguire il calcolo, visto che il file è abbastanza larga.\n",
    "\n",
    "Trasforma la riga seguente in codice in colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e77f4",
   "metadata": {},
   "source": [
    "\n",
    "!wget https://raw.githubusercontent.com/fdalforno/nlp/master/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "095159f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst']\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "# get list of words\n",
    "words = utils.preprocess(text)\n",
    "print(words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9cf3ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in text: 16680599\n",
      "Unique words: 63641\n"
     ]
    }
   ],
   "source": [
    "# print some stats about this word data\n",
    "print(\"Total words in text: {}\".format(len(words)))\n",
    "print(\"Unique words: {}\".format(len(set(words)))) # `set` removes any duplicate words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9f7089",
   "metadata": {},
   "source": [
    "### Dizionari\n",
    "Ora, Andiamo a creare due dizionari per convertire le parole in interi e il contrario (da interi e parole). Questo viene eseguito ancora nel file `utils.py`. La funzione `create_lookup_tables` prende una lista di parole di un testo e crea due dizionari.\n",
    "\n",
    "Gli interi sono assegnati in ordine decrescente di frequenza, dunque la parola più frequente (\"the\") prende il token 0, la parola prossima più frequente prende il codice 1 e così via.\n",
    "\n",
    "Una volta che abbiamo ottenuto i nostri dizionari, le parole vengono convertite in interi e salvate nella lista `int_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33892f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5233, 3080, 11, 5, 194, 1, 3133, 45, 58, 155, 127, 741, 476, 10571, 133, 0, 27349, 1, 0, 102, 854, 2, 0, 15067, 58112, 1, 0, 150, 854, 3580]\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int, int_to_vocab = utils.create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]\n",
    "\n",
    "print(int_words[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401857a5",
   "metadata": {},
   "source": [
    "## Subsampling\n",
    "\n",
    "Le parole che compaiono spesso come \"the\", \"of\" e \"for\" non forniscono molto contesto alle parole vicine.\n",
    "Se scartiamo alcune di esse, possiamo rimuovere un pò del rumore dai dati e come risultato otteniamo anche che la fase di training risulta più veloce con una migliore rappresentazione. Questo processo è chiamato da Mikolov *subsampling*.\n",
    "\n",
    "Per ogni parola $w_i$ nel dataset di training, le scarteremo con la probabilità data da:\n",
    "\n",
    "$$ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}} $$\n",
    "\n",
    "dove $t$ è un parametro di threshold e $f(w_i)$ è la frequenza della parola $w_i$ nel dataset.\n",
    "\n",
    "$$ P(0) = 1 - \\sqrt{\\frac{1*10^{-5}}{1*10^6/16*10^6}} = 0.98735 $$\n",
    "\n",
    "Implementiamo il subsampling per le parole `int_words`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47347362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "threshold = 1e-5\n",
    "word_counts = Counter(int_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b696a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 325873)\n"
     ]
    }
   ],
   "source": [
    "print(list(word_counts.items())[3])  # dictionary of int_words, how many times they appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e193a12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = len(int_words)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd6db20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard some frequent words, according to the subsampling equation\n",
    "# create a new list of words for training\n",
    "train_words = [word for word in int_words if random.random() < (1 - p_drop[word])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6280fbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5233, 3133, 10571, 27349, 102, 854, 15067, 58112, 3580, 10712, 3672, 708, 7088, 247, 44611, 5233, 8983, 4147, 6437, 4186, 362, 5233, 447, 1818, 6753, 7573, 1774, 566, 11064, 89]\n"
     ]
    }
   ],
   "source": [
    "print(train_words[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4304adda",
   "metadata": {},
   "source": [
    "## creare batches\n",
    "\n",
    "Ora che i nostri dati sono ben sistemati, dobbiamo preparli per farli passare attraverso la nostra rete. Con l'architettura skip-gram , per ogni parola del testo, vogliamo definire un _contesto_ e cattura tutte le parole in una finestra attorno alla parola di dimensione $C$. \n",
    "\n",
    "Da [Mikolov et al.](https://arxiv.org/pdf/1301.3781.pdf):\n",
    "\n",
    "\"Visto che le parole più distanti sono solitamente meno collegate alla parola corrente che alle parole più vicine, diamo meno peso alle parole distanti campionando meno frequentemente queste parole nei dati di training. Se scegliamo ad esempio $C = 5$, per ogni parola di trainig sceglieremo un numero random $R$ nel range $[ 1: C ]$, e poi useremo le $R$ parole a sinistra e a destra della parola corrente come etichetta corretta.\"\n",
    "\n",
    "Implementiamo la funzione `get_target`  ad esempio se abbiamo la seguente lista prendiamo idx=2, `741`: \n",
    "\n",
    "```\n",
    "[5233, 58, 741, 10571, 27349, 0, 15067, 58112, 3580, 58, 10712]\n",
    "```\n",
    "\n",
    "Per `R=2`, `get_target` ritorna questi quattro valori:\n",
    "```\n",
    "[5233, 58, 10571, 27349]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3acae62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(words, idx, window_size=5):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    \n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    target_words = words[start:idx] + words[idx+1:stop+1]\n",
    "    \n",
    "    return list(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b3d484a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Target:  [2, 3, 4, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "# test your code!\n",
    "\n",
    "# run this cell multiple times to check for random window selection\n",
    "int_text = [i for i in range(10)]\n",
    "print('Input: ', int_text)\n",
    "idx=5 # word index of interest\n",
    "\n",
    "target = get_target(int_text, idx=idx, window_size=5)\n",
    "print('Target: ', target)  # you should get some indices around the idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3637a71",
   "metadata": {},
   "source": [
    "### Generazione dei batch\n",
    "\n",
    "Qui abbiamo un generatore che ritorna dei batch con dati di input e target per il nostro modello usando la funzione `get_target`\n",
    "costruita sopra.\n",
    "\n",
    "L'idea è quella di costruire un insieme di parole di dimensione `batch_size` da una lista di parole.\n",
    "Poi per ogniuno di questi batch si ottiene la parola target in una finestra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa86b702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(words, batch_size, window_size=5):\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71cf8096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [0, 0, 0, 1, 1, 2, 2, 2, 3, 3, 3]\n",
      "y\n",
      " [1, 2, 3, 0, 2, 0, 1, 3, 0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "int_text = [i for i in range(20)]\n",
    "x,y = next(get_batches(int_text, batch_size=4, window_size=5))\n",
    "\n",
    "print('x\\n', x)\n",
    "print('y\\n', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12df1a2c",
   "metadata": {},
   "source": [
    "## Costruzione del grafo\n",
    "\n",
    "Qui sotto una approssimazione del diagramma della struttura della nostra rete. \n",
    "\n",
    "<img src=\"./images/skip_gram_arch.png\" width=60%>\n",
    "\n",
    "* le parole sono passate come batch di token codificati come interi \n",
    "* queste attraversano un hidden layer il nostro embedding layer.\n",
    "* alla fine usiamo il layer di output con la parte softmax\n",
    "\n",
    "Useremo il layer softmax per eseguire delle previsioni circa la parola del contesto come definito.\n",
    "L'idea è quella di addestrare la nostra rete per trovare una rappresentazione efficiente delle nostre parole.\n",
    "\n",
    "Possiamo dimenticarci dell'ultimo layer softmax non ci importa molto di eseguire delle previsioni con questa rete.\n",
    "Volgiamo soltanto usare la matrice di embedding che verrà usata in _altre_ reti neuronali.\n",
    "\n",
    "---\n",
    "\n",
    "## Validazione\n",
    "Qui stiamo creando una funzione che ci aiuterà ad osservare cosa il nostro modello sta imparando.\n",
    "Dobbiamo scegliere alcune parole comuni e alcune parole non comuni.\n",
    "\n",
    "\n",
    "Poi mostreremo le parole più vicine alle parole selezionate usando il coseno di similitudine:\n",
    "\n",
    "<img src=\"./images/two_vectors.png\" width=30%>\n",
    "\n",
    "$$\n",
    "\\mathrm{similarity} = \\cos(\\theta) = \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}||\\vec{b}|}\n",
    "$$\n",
    "\n",
    "\n",
    "Possiamo codificare le parole di validazione come vettori $\\vec{a}$ usando la tebella di embedding, poi calcoliamo la similarità per ogni word vector $\\vec{b}$  nella tabella di embedding.\n",
    "\n",
    "Con queste similitudini possiamo mostrare le parole di validazione e le parole nella tabella di embedding semanticamente simili alla stessa. Questo è un modo carino per controllare se l'operazione di embedding sta raggruppando parole con un significato semantico simile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bd2a456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(embedding, valid_size=16, valid_window=100, device='cpu'):\n",
    "    \"\"\" Returns the cosine similarity of validation words with words in the embedding matrix.\n",
    "        Here, embedding should be a PyTorch embedding module.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Here we're calculating the cosine similarity between some random words and \n",
    "    # our embedding vectors. With the similarities, we can look at what words are\n",
    "    # close to our random words.\n",
    "    \n",
    "    # sim = (a . b) / |a||b|\n",
    "    \n",
    "    embed_vectors = embedding.weight\n",
    "    \n",
    "    # magnitude of embedding vectors, |b|\n",
    "    magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0)\n",
    "    \n",
    "    # pick N words from our ranges (0,window) and (1000,1000+window). lower id implies more frequent \n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples,\n",
    "                               random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "    valid_examples = torch.LongTensor(valid_examples).to(device)\n",
    "    \n",
    "    valid_vectors = embedding(valid_examples)\n",
    "    similarities = torch.mm(valid_vectors, embed_vectors.t())/magnitudes\n",
    "        \n",
    "    return valid_examples, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f1cd00",
   "metadata": {},
   "source": [
    "## SkipGram model\n",
    "\n",
    "\n",
    "Definiamo il modello SkipGram.\n",
    "> Abbiamo bisgono di definire un [embedding layer](https://pytorch.org/docs/stable/nn.html#embedding) e nella parte finale un layer softmax.\n",
    "\n",
    "Il layer di Embedding vuole i seguenti parametri:\n",
    "* **num_embeddings** - la dimensione del dizionario di embeddings, o quante righe vogliamo nella nostra matrice di embedding.\n",
    "* **embedding_dim** - la dimensione di ogni vettore; ovvero la dimensione di embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bae71793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8675ce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, n_vocab, n_embed):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(n_vocab, n_embed)\n",
    "        self.output = nn.Linear(n_embed, n_vocab)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        scores = self.output(x)\n",
    "        log_ps = self.log_softmax(scores)\n",
    "        \n",
    "        return log_ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb66aa9",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d814f09",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DALFOR~1\\AppData\\Local\\Temp/ipykernel_2440/1228306252.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mlog_ps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_ps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\DALFOR~1\\AppData\\Local\\Temp/ipykernel_2440/4122232662.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mlog_ps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlog_ps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m   1445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1446\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1447\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1449\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"log_softmax\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1922\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1924\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1925\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "embedding_dim=300 # you can change, if you want\n",
    "\n",
    "model = SkipGram(len(vocab_to_int), embedding_dim).to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "print_every = 500\n",
    "steps = 0\n",
    "epochs = 5\n",
    "\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    \n",
    "    # get input and target batches\n",
    "    for inputs, targets in get_batches(train_words, 512):\n",
    "        steps += 1\n",
    "        inputs, targets = torch.LongTensor(inputs), torch.LongTensor(targets)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        log_ps = model(inputs)\n",
    "        loss = criterion(log_ps, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if steps % print_every == 0:                  \n",
    "            # getting examples and similarities      \n",
    "            valid_examples, valid_similarities = cosine_similarity(model.embed, device=device)\n",
    "            _, closest_idxs = valid_similarities.topk(6) # topk highest similarities\n",
    "            \n",
    "            valid_examples, closest_idxs = valid_examples.to('cpu'), closest_idxs.to('cpu')\n",
    "            for ii, valid_idx in enumerate(valid_examples):\n",
    "                closest_words = [int_to_vocab[idx.item()] for idx in closest_idxs[ii]][1:]\n",
    "                print(int_to_vocab[valid_idx.item()] + \" | \" + ', '.join(closest_words))\n",
    "            print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3529d26a",
   "metadata": {},
   "source": [
    "## Visualizzare i word vector\n",
    "\n",
    "Useremo il metodo T-SNE per visualizzare come i nostri word vector in alta dimensionalità si raggreuppano assieme.\n",
    "T-SNE viene usato per proiettare questi vettori in uno spazio bidimensionale preservando le strutture locale. \n",
    "\n",
    "Per avere più informazioni circa questo metodo possiamo documentarci leggendo questo [post di Christopher Olah](http://colah.github.io/posts/2014-10-Visualizing-MNIST/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f120dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceeaa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting embeddings from the embedding layer of our model, by name\n",
    "embeddings = model.embed.weight.to('cpu').data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b613747",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_words = 600\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embeddings[:viz_words, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aa8c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='steelblue')\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4b0282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
