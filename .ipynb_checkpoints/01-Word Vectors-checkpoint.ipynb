{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3ea8b8b",
   "metadata": {},
   "source": [
    "# Introduzione al word vector\n",
    "\n",
    "[link al corso](https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ)\n",
    "[libro](https://github.com/practical-nlp/practical-nlp-code)\n",
    "\n",
    "\n",
    "\n",
    "## Cosa speriamo di imparare\n",
    "\n",
    "* le fondamenta dei metodi di deep learning applicati al problema NLP\n",
    "* una immagine di insieme sulla comprensione del linguaggio umano e sulla difficoltà di comprensione e rappresentazione in un computer\n",
    "* la comprensione di come costruire sistemi tramite (PyTorch) per alcuni dei maggiori problemi di NLP come ad esempio: \n",
    "    * Word meaning, \n",
    "    * dependency parsing, \n",
    "    * machine translation, \n",
    "    * question answering\n",
    "\n",
    "Il linguaggio umano è un sistema sociale, costruito e interpretato dalle persone.\n",
    "Non è un sistema formale bensi un glorioso caos, costruito dagli umani.\n",
    "\n",
    "Il primo passo interessante è stato il modello GPT-3 perchè è il primo oggetto che potremmo chiamare modello universale.\n",
    "Questo modello estremamente grande è stato addestrato su una serie di dati molto grande (wikipedia) e al suo interno contiene infomazioni circa l'attività che deve svolgere (contesto), si può dunque usarlo per fargli eseguire molte attività.\n",
    "\n",
    "In questo modo non dobbiamo costruire ogni volta un modello per rilevare lo spam, linguaggio non consueto, un modello per ogni lingua. \n",
    "Abbiamo uno strumento che capisce da solo cosa fare.\n",
    "\n",
    "Tutto questo partendo da un modello che esegue la previsione di una parola che segue delle altre parole.\n",
    "\n",
    "## Come rappresentiamo il significato di un testo/parola?\n",
    "\n",
    "Definizione di **significato**:\n",
    "\n",
    "* L'idea che è rappresentata da una parola, frase ecc ecc\n",
    "* L'idea che una persona vuole esprimere usando parole, segni ecc ecc \n",
    "* L'idea che viene espressa in un lavoro di scrittura, arte ecc ecc\n",
    "    \n",
    "Il modo linguistico più comune per rappresentare il significato:\n",
    "\n",
    "significante (simbolo) $\\leftrightarrow$ significato (idea o cosa)\n",
    "\n",
    "![significato](./images/significato.png)\n",
    "\n",
    "questo modello non è facilmente implementabile in un computer ad esempio come posso implementare in un computer il concetto/simbolo di una sedia?\n",
    "\n",
    "Quindi tradizionalmente il modo in cui il concetto è stato rappresentato normalmente gestito nei sistemi di elborazione del linguaggio naturale è stato quello di utilizzare risorse come ad esempio un dizionario.\n",
    "\n",
    "Questi dizionari contengono liste di insiemi di sinonimi e [iperonimi](https://www.treccani.it/enciclopedia/iperonimi_%28Enciclopedia-dell%27Italiano%29/).\n",
    "\n",
    "qui un insieme di sinonimi per \"good\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab38589c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun: good\n",
      "noun: good, goodness\n",
      "noun: good, goodness\n",
      "noun: commodity, trade_good, good\n",
      "adj: good\n",
      "adj (s): full, good\n",
      "adj: good\n",
      "adj (s): estimable, good, honorable, respectable\n",
      "adj (s): beneficial, good\n",
      "adj (s): good\n",
      "adj (s): good, just, upright\n",
      "adj (s): adept, expert, good, practiced, proficient, skillful, skilful\n",
      "adj (s): good\n",
      "adj (s): dear, good, near\n",
      "adj (s): dependable, good, safe, secure\n",
      "adj (s): good, right, ripe\n",
      "adj (s): good, well\n",
      "adj (s): effective, good, in_effect, in_force\n",
      "adj (s): good\n",
      "adj (s): good, serious\n",
      "adj (s): good, sound\n",
      "adj (s): good, salutary\n",
      "adj (s): good, honest\n",
      "adj (s): good, undecomposed, unspoiled, unspoilt\n",
      "adj (s): good\n",
      "adv: well, good\n",
      "adv: thoroughly, soundly, good\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "word = \"good\"\n",
    "#word = \"ninja\"\n",
    "\n",
    "poses = { 'n':'noun', 'v':'verb', 's':'adj (s)', 'a':'adj', 'r':'adv'}\n",
    "for synset in wn.synsets(word):\n",
    "    print(\"{}: {}\".format(poses[synset.pos()],\", \".join([l.name() for l in synset.lemmas()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e85d91",
   "metadata": {},
   "source": [
    "qui gli iperonimi di \"panda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e02bd908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('procyonid.n.01'),\n",
       " Synset('carnivore.n.01'),\n",
       " Synset('placental.n.01'),\n",
       " Synset('mammal.n.01'),\n",
       " Synset('vertebrate.n.01'),\n",
       " Synset('chordate.n.01'),\n",
       " Synset('animal.n.01'),\n",
       " Synset('organism.n.01'),\n",
       " Synset('living_thing.n.01'),\n",
       " Synset('whole.n.02'),\n",
       " Synset('object.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('entity.n.01')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "panda = wn.synset(\"panda.n.01\")\n",
    "hyper = lambda s: s.hypernyms()\n",
    "list(panda.closure(hyper))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fef045",
   "metadata": {},
   "source": [
    "Questa è un'ottima risorsa per i problemi di NLP ma abbiamo un problemino.\n",
    "Non riusciamo a catturare alcune sfumature:\n",
    "\n",
    "* ad esempio “proficient” (competente) è segnato come sinonimo per \"good\" (buono) ma questo è vero solo in alcuni contesti\n",
    "* questo sistema elenca i sinonimi offensivi senza nessuna copertura delle annotazioni o appropriatezza delle parole\n",
    "* mancano i significati delle nuove parole ad esempio: wicked, badass, nifty, wizard, genius, ninja, bombest è impossibile tenerlo sempre aggiornato\n",
    "* è soggettivo\n",
    "* richiede del lavoro umano per aggiornarlo e adattarlo\n",
    "* non può essere usato per ottenere parole simili con accuratezza\n",
    "\n",
    "## Rappresentazione del testo \n",
    "\n",
    "Come abbiamo intuito da prima il testo deve essere convertito in una rappresentazione matematica, rappresenteremo il testo come vettori di numeri.\n",
    "\n",
    "Vogliamo inoltre che dati due vettori $A$ e $B$ definiamo il concetto di similarità come il coseno di similitudine:\n",
    "\n",
    "$$ \\text{similarity} = \\cos(\\theta ) = \\frac{A \\cdot B}{\\lVert  A  \\rVert_2 \\lVert  B  \\rVert_2 }$$\n",
    "\n",
    "\n",
    "![similarity](./images/two_vectors.png)\n",
    "\n",
    "creiamo un dataset giocattolo per capire il concetto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36c9f5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
    "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
    "processed_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a693ac1a",
   "metadata": {},
   "source": [
    "dal testo mi creo il vocabolario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ba74581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dog': 1, 'bites': 2, 'man': 3, 'eats': 4, 'meat': 5, 'food': 6}\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "count = 0\n",
    "for doc in processed_docs:\n",
    "    for word in doc.split():\n",
    "        if word not in vocab:\n",
    "            count = count +1\n",
    "            vocab[word] = count\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1923e252",
   "metadata": {},
   "source": [
    "nei problemi di NLP consideriamo le parole come simboli discreti, ogni simbolo di una parola è rappresentata da un vettore con il valore 1 in una posizione (vedi le parole sopra) e zero in tutti gli altri elementi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a18f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot_vector(somestring):\n",
    "    onehot_encoded = []\n",
    "    for word in somestring.split():\n",
    "        temp = [0]*len(vocab)\n",
    "        if word in vocab:\n",
    "            temp[vocab[word]-1] = 1 # -1 is to take care of the fact indexing in array starts from 0 and not 1\n",
    "        onehot_encoded.append(temp)\n",
    "    return onehot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63ebd1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man bites dog\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(processed_docs[1])\n",
    "get_onehot_vector(processed_docs[1]) #one hot representation for a text from our corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc76cfb",
   "metadata": {},
   "source": [
    "come si vede il primo problema è legato al fatto che il vocabolario può essere molto grande e il nostro vettore può arrivare ad avere dimensione anche di 500000 elementi.\n",
    "\n",
    "Inoltre questa rappresentazione presenta un ulteriore problema, non riusciamo a rappresentare una relazione tra le parole (similitudine) in quanto i vettori sono ortogonali.\n",
    "\n",
    "Dobbiamo trovare un sistema per codificare la similitudine nei vettori.\n",
    "\n",
    "## Rappresentare le parole dal loro contesto\n",
    "Questo ci porta alla [semantica distribuzionale](https://it.wikipedia.org/wiki/Semantica_distribuzionale) in cui diciamo che il **significato di una parola** viene dato dalle parole che spesso appaiono nelle sue vicinanze.\n",
    "\n",
    "    Conoscerai una parola dalla compagnia che frequenta.\n",
    "     J. R. Firth (1890 – 1960)\n",
    "\n",
    "Rappresenta una delle idee di successo all'approccio moderno alla NLP.\n",
    "Quando una parola *w* appare nel testo il suo **contesto** è l'insieme di parole che appaiono nelle vicinanze, in una finestra di dimensione fisse\n",
    "\n",
    "![contesto](./images/context.png)\n",
    "\n",
    "\n",
    "L'oggetto che andremo a costruire sarà rappresentato dal *word vector* un vettore denso per ogni parola scelto in modo che parole simili ottengano vettori simili come ad esempio :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8be82e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8570493105815338"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "banking = np.asarray([0.286,0.792,-0.177,-0.107,0.109,-0.542,0.349,0.271])\n",
    "monetary = np.asarray([0.413,0.582,-0.007,0.247,0.216,-0.718,0.147,0.051])\n",
    "\n",
    "np.dot(banking, monetary) / (np.linalg.norm(banking) * np.linalg.norm(monetary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d75acf4",
   "metadata": {},
   "source": [
    "i vettori sopra sono a 8 dimensioni per semplicità ma di solito si arriva ad una dimensione pari a 300\n",
    "i word vectors sono anche chiamati embeddings o word representations, sono rappresentazioni distribuite.\n",
    "\n",
    "## Introduzione a word2vec\n",
    "\n",
    "iniziamo ad introdurre l'algoritmo Word2vec (Mikolov et al. 2013) che rappresenta il primo sistema per estrarre dal testo questi vettori.\n",
    "\n",
    "L'algoritmo parte dall'idea che abbiamo a disposizione un sacco di testo che chiameremo (corpus) da quello per per ogni parola parola in un vocabolario fisso viene rappresentata da un vettore. \n",
    "\n",
    "Passiamo poi ogni posizione del testo *t*, con una parola centrale *c* ed un contesto di parole *o*. \n",
    "Usiamo la somiglianza dei word vector di *c* e *o* per calcolare la probabilità di *o* dato *c* o viceversa\n",
    "\n",
    "Aggiustiamo i valori del word vector al fine di massimizzare la probabilità.\n",
    "Otterremo dunque un insieme di vettori uno per ogni parola di quel testo.\n",
    "\n",
    "Qui sotto un esempio del calcolo della probabilità $P(w_{t+j}|w_t)$ (si legge Probabilità di $w_{t+j}$ dato $w_t$)\n",
    "![window](./images/window.png)\n",
    "\n",
    "\n",
    "### Funzione obiettivo\n",
    "Il nostro scopo è quello di creare un modello i cui parametri siano i **word vectors**, poi si esegue l'addestramento del modello su un certo obiettivo.\n",
    "Ad ogni iterazione eseguiamo il nostro modello, valutiamo gli errori ed seguiamo una regola che conosce come penalizzare i parametri del modello che hanno creato questi errori. Alla fine abbiamo i nostri **word vectors**.\n",
    "\n",
    "Sono stati provati molti approcci, qui vediamo un approccio *word2vec* che include:\n",
    "\n",
    "* 2 algoritmi continuous bag-of-words (CBOW) e skip-gram, CBOW mira a predirre la parola centrale partendo dal contesto, mentre skip-gram fa esattamente il contrario, prevede la distribuzione (probabilità) delle parole del contesto partendo dalla parola centrale\n",
    "\n",
    "* 2 sistemi di training negative sampling e hierarchical softmax. Nel negative sampling definiamo un obiettivo campionando gli esempi negativi mentre nel hierarchical softmax usiamo una funzione ad albero efficiente per calcolare la probabilità di ogni parola nel vocabolario.\n",
    "\n",
    "Dobbiamo dunque creare un modello che assegna una probabilità ad ogni sequenza di token. \n",
    "Prendiamo la frase *\"The cat jumped over the puddle.\"* un buon modello darà una probabilità alta alla seguente frase in quanto la frase è valida (sintatticamente e semanticamente).\n",
    "\n",
    "Similmente il modello dovrà dare una bassa probabilità alla frase *\"stock boil fish is toy\"* in quanto questa non ha senso.\n",
    "Matematicamente possiamo scrivere questa sequenza di parole come:\n",
    "\n",
    "$$P(w_1,w_2,...,w_n)$$\n",
    "\n",
    "Per calcolare questa probabilità possiamo partire da un modello semplice detto unigram il quale assume che le parole non sono correlate tra di loro:\n",
    "\n",
    "$$P(w_1,w_2,...,w_n) = \\prod_{i=1}^{n} P(w_{i})$$\n",
    "\n",
    "Tuttavia si vede subito che questo modello è ridicolo in quanto la parola seguente dipende fortemente dalle parole antecedenti.\n",
    "Infatto la seconda frase con questo modello avrà un punteggio elevato. \n",
    "\n",
    "Cambiamo modello e definiamo che la probabilità della frase dipende dalle probabilità delle coppie nella frase (parola antecedente e successiva), chiamiamo questo modello bigram\n",
    "\n",
    "$$P(w_1,w_2,...,w_n) = \\prod_{i=2}^{n} P(w_{i}|w_{i-1})$$\n",
    "\n",
    "Anche questo è un modello un pochino ingenuo, ci stiamo concentrando solo sulle coppie e non sull'intera frase.\n",
    "Vedremo più avanti però che questa rappresentazione può essere espansa.\n",
    "\n",
    "\n",
    "### Continuous Bag of Words Model (CBOW)\n",
    "\n",
    "Prendendo la frase *\"The cat jumped over the puddle.\"* il primo approccio è quello di predire la parola *jumped* partendo dal contesto *[\"the\",\"cat\",\"over\",\"the\"]* questo modello viene chiamato continuous bag of word.\n",
    "\n",
    "Vediamo i parametri conosciuti del modello, in ingresso ed in uscita abbiamo le parole codificate tramite *one-hot vectors*. I vettori in ingresso (contesto) verranno rappresentati con il simbolo $x^{c}$, l'output verrà rappresentato come $y^{c}$.\n",
    "Nel modello CBOW visto che il risultato è univoco possiamo scrivere tranquillamente il risultato come $y$.\n",
    "\n",
    "Definiamo ora le variabili sconosciute del modello, creiamo due matrici, $\\mathcal{V} \\in \\mathbb{R}^{n \\times |V|}$ e $\\mathcal{U} \\in \\mathbb{R}^{|V| \\times n}$, $n$ viene scelto in modo arbitrario e rappresenta il nostro spazio di embeddings.\n",
    "\n",
    "$\\mathcal{V}$ rappresenta la matrice delle parole di input tale che la $i$-esima colonna di $\\mathcal{V}$ è la rappresentazione a dimensione $n$ della parola $w_{i}$ quando questa è un input del modello. \n",
    "\n",
    "Si denota questo vettore come $\\mathcal{v}_i$\n",
    "\n",
    "Similmente $\\mathcal{U}$ rappresenta la matrice delle parole in output la $j$-esima colonna di $\\mathcal{U}$ rappresenta il vettore di embeddings $\\mathcal{u}_j$ quando la parola $w_{j}$ è un output del modello.\n",
    "\n",
    "Notiamo subito che andremo a creare due vettori per rappresentare la stessa parola $w_{i}$ ($\\mathcal{v}_i$ e  $\\mathcal{u}_i$).\n",
    "Analizziamo come funziona il modello in questi passaggi:\n",
    "\n",
    "\n",
    "1. andiamo a generare un one-hot vectors per ogni parola del contesto di input di dimensione m: $(x^{c-m},\\cdots,x^{c-1},x^{c+1},x^{c+m}) \\in  \\mathbb{R}^{|V|}$\n",
    "\n",
    "2. andiamo ad ottenere i word-vectors del contesto moltiplicando i vettori $x$ per $\\mathcal{V}$\n",
    "\n",
    "3. eseguiamo la media e otteniamo il vettore $\\mathcal{\\hat{v}}$\n",
    "\n",
    "4. generiamo il vettore dei punteggi $\\mathcal{z} = \\mathcal{U} \\cdot \\mathcal{\\hat{v}}$ \n",
    "\n",
    "5. trasformiamo i punteggi in probabilità $\\hat{y} = softmax(z) \\in \\mathbb{R}^{|V|}$.\n",
    "\n",
    "6. quello che desideriamo è che le probabilità generate $\\hat{y}$ vadano a combaciare con la probabilità vera $y$ il vettore one-hot della parola attuale\n",
    "\n",
    "Abbiamo compreso ora come sarebbe il modello se avessimo $\\mathcal{U}$ e $\\mathcal{V}$ ma come si ottengono queste due matrici?\n",
    "Prima cosa dobbiamo creare la funzione obiettivo, in questo caso siamo fortunati ci troveremo spesso a dover trattare una probabilità calcolata e una probabilità reale.\n",
    "\n",
    "Come si misura la distanza tra due distibuzioni? \n",
    "La risposta sta nella cross-entropy (entropia incrociata) $H(\\hat{y},y)$.\n",
    "\n",
    "Nel caso discreto la formula \n",
    "\n",
    "$$H(\\hat{y},y) = -\\sum_{j=1}^{|V|}y_j log(\\hat{y}_j)$$\n",
    "\n",
    "Occupiamoci del caso in questione, con $y$ vettore one-hot che semplifica la formula:\n",
    "\n",
    "$$H(\\hat{y},y) = -y_j log(\\hat{y}_j)$$\n",
    "\n",
    "Partiamo con il caso in cui la previsione sia corretta, diciamo che $c$ è l'indice della parola nel vettore one-hot che vogliamo ottenere dunque se la previsione è corretta avremo che $\\hat{y}_c = 1$. In questo caso la formula sarà:\n",
    "\n",
    "$$H(\\hat{y},y) = -1 log(1) = 0$$\n",
    "\n",
    "Per una previsione corretta abbiamo una penalità pari a 0 (perfetto), ora consideriamo il caso opposto una previsione completamente errata con $\\hat{y}_c = 0.001$.\n",
    "\n",
    "$$H(\\hat{y},y) = -1 log(0.01) \\approx 4.605$$\n",
    "\n",
    "Come possiamo vedere questa formula è una buona misura per la distanza tra le due distribuzioni, creiamo dunque la nostra funzione obiettivo:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "    minimizzare J & = -log(P(w_c|w_{c-m},\\cdots,w_{c-1},w_{c+1},\\cdots,w_{c+m})) \\\\\n",
    "    & = -log(P(u_c|\\hat{v})) \\\\\n",
    "    & = -log \\left(\\frac{exp(u_c^{T}\\hat{v})}{\\sum_{j=1}^{|V|}exp(u_j^{T}\\hat{v})} \\right) \\\\\n",
    "    & = -exp(u_c^{T}\\hat{v}) + \\sum_{j=1}^{|V|}exp(u_j^{T}\\hat{v})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "![CBOW](./images/cbow.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2a8e474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 's3.amazonaws.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78008c588d1b4302bc2b585087004bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31344016 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "dataset = \"https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bbe6499_text8/text8.zip\"\n",
    "folder_result = \"./data/\" \n",
    "\n",
    "with requests.get(dataset, stream=True, verify=False) as r:\n",
    "    total_length = int(r.headers.get(\"Content-Length\"))\n",
    "    with tqdm.wrapattr(r.raw, \"read\", total=total_length, desc=\"\")as raw:\n",
    "        with open(f\"{os.path.join(folder_result,os.path.basename(r.url))}\", 'wb') as output:\n",
    "            shutil.copyfileobj(raw, output)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b7eac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"./data/text8.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(folder_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b4b0880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including t\n"
     ]
    }
   ],
   "source": [
    "# read in the extracted text file      \n",
    "with open('data/text8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# print out the first 100 characters\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bab1e113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst']\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "# get list of words\n",
    "words = utils.preprocess(text)\n",
    "print(words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b5e8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
